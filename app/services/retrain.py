# # app/services/retrain.py
"""
–ú–æ–¥—É–ª—å –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ (feedback)

–ú–æ–¥—É–ª—å —Ä–µ–∞–ª–∏–∑—É–µ—Ç:
- –ó–∞–≥—Ä—É–∑–∫—É —Ñ–∏–¥–±—ç–∫–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞
- –í–∞–ª–∏–¥–∞—Ü–∏—é —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
- –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É –∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
- –î–æ–æ–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª–µ–≤–æ–π –º–æ–¥–µ–ª–∏ (VotingClassifier)
- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±–Ω–æ–≤–ª—ë–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ background_data

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ —ç–Ω–¥–ø–æ–∏–Ω—Ç–µ /retrain

–ê–≤—Ç–æ—Ä: [–ö–æ—á–Ω–µ–≤–∞ –ê—Ä–∏–Ω–∞]
–ì–æ–¥: 2025
"""

import pandas as pd
import joblib
import json
import logging
from pathlib import Path
from sklearn.ensemble import VotingClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from sklearn.metrics import accuracy_score

from shared.data_processing import preprocess_data_for_prediction
from shared.config import (
    FEEDBACK_PATH,
    FEATURE_NAMES_PATH,
    BACKGROUND_DATA_PATH,
    ENSEMBLE_MODEL_PATH
)


logger = logging.getLogger(__name__)


def retrain_model_from_feedback():
    """
    –ü–µ—Ä–µ–æ–±—É—á–∞–µ—Ç –∞–Ω—Å–∞–º–±–ª–µ–≤—É—é –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö
    —Ñ–∏–¥–±—ç–∫–æ–≤ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π.

    –ü—Ä–æ—Ü–µ—Å—Å –≤–∫–ª—é—á–∞–µ—Ç:
        1. –ó–∞–≥—Ä—É–∑–∫—É –∏ –ø–∞—Ä—Å–∏–Ω–≥ feedback.jsonl
        2. –í–∞–ª–∏–¥–∞—Ü–∏—é —Å—Ç—Ä—É–∫—Ç—É—Ä—ã, —Ç–∏–ø–æ–≤ –∏ –±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤
        3. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö (OHE, feature engineering)
        4. –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª—å—é
        5. –î–æ–æ–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è (RF + XGBoost + CatBoost)
        6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ background_data

    –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ joblib. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è soft voting –¥–ª—è
    —É—Å—Ä–µ–¥–Ω–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π.

    Returns:
        dict: –†–µ–∑—É–ª—å—Ç–∞—Ç –¥–æ–æ–±—É—á–µ–Ω–∏—è:
            {
                "status": "retrained",
                "samples_used": 12,
                "model_path": "/–ø–æ–ª–Ω—ã–π/–ø—É—Ç—å/–∫/ensemble_model.pkl",
                "accuracy_on_feedback": 0.917,
                "class_balance": {0: 0.55, 1: 0.45}
            }

    Raises:
        FileNotFoundError: –ï—Å–ª–∏ feedback.jsonl –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        ValueError: –ü—Ä–∏ –æ—à–∏–±–∫–∞—Ö –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            (–ø—É—Å—Ç–æ–π —Ñ–∞–π–ª, –Ω–µ—Ö–≤–∞—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö, –¥–∏—Å–±–∞–ª–∞–Ω—Å)
        Exception: –ü—Ä–∏ –æ—à–∏–±–∫–∞—Ö –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–ª–∏ –æ–±—É—á–µ–Ω–∏—è

    –ü—Ä–∏–º–µ—á–∞–Ω–∏—è:
        - –§–∏–¥–±—ç–∫–∏ —Ö—Ä–∞–Ω—è—Ç—Å—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSONL (–æ–¥–∏–Ω JSON-–æ–±—ä–µ–∫—Ç –Ω–∞ —Å—Ç—Ä–æ–∫—É)
        - actual_status: 0 ‚Äî repaid, 1 ‚Äî default
        - –ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–ª–∏ —Å–æ–∑–¥–∞—ë—Ç—Å—è –Ω–æ–≤–∞—è –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏
        - background_data –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –¥–ª—è SHAP-–æ–±—ä—è—Å–Ω–µ–Ω–∏–π

    –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
        >>> result = retrain_model_from_feedback()
        >>> print(f"–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ñ–∏–¥–±—ç–∫–∞—Ö: {result['accuracy_on_feedback']:.3f}")
    """
    # --- 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞ ---
    if not FEEDBACK_PATH.exists():
        raise FileNotFoundError(
            f"–§–∞–π–ª —Ñ–∏–¥–±—ç–∫–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω: {FEEDBACK_PATH}"
        )

    # --- 2. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–∞—Ä—Å–∏–Ω–≥ JSONL ---
    try:
        lines = FEEDBACK_PATH.read_text(
            encoding="utf-8"
        ).strip().split("\n")
        lines = [line.strip() for line in lines if line.strip()]
        if len(lines) == 0:
            raise ValueError(
                "–§–∞–π–ª feedback.jsonl –ø—É—Å—Ç. –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è."
            )

        df = pd.DataFrame([json.loads(line) for line in lines])
        logger.info(
            f"üì• –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π –∏–∑ {FEEDBACK_PATH.name}"
        )

    except json.JSONDecodeError as e:
        raise ValueError(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {str(e)}")
    except Exception as e:
        raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∏–¥–±—ç–∫–∏: {str(e)}")

    # --- 3. –í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã ---
    required_cols = [
        'person_age', 'person_income', 'person_home_ownership',
        'person_emp_length', 'loan_intent', 'loan_grade',
        'loan_amnt', 'loan_int_rate', 'loan_percent_income',
        'cb_person_default_on_file', 'cb_person_cred_hist_length',
        'actual_status'
    ]
    missing = [col for col in required_cols if col not in df.columns]
    if missing:
        raise ValueError(
            f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing}"
        )

    # --- 4. –í–∞–ª–∏–¥–∞—Ü–∏—è —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π ---
    if not pd.api.types.is_numeric_dtype(df['actual_status']):
        raise ValueError(
            "–ö–æ–ª–æ–Ω–∫–∞ 'actual_status' –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —á–∏—Å–ª–æ–≤–æ–π (0 –∏–ª–∏ 1)"
        )

    unique_statuses = set(df['actual_status'].unique())
    if not unique_statuses.issubset({0, 1}):
        raise ValueError(
            f"–î–æ–ø—É—Å—Ç–∏–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è actual_status: 0 (repaid), "
            f"1 (default). –ù–∞–π–¥–µ–Ω—ã: {unique_statuses}"
        )

    # --- 5. –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ ---
    min_samples = 5
    if len(df) < min_samples:
        raise ValueError(
            f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è. "
            f"–ú–∏–Ω–∏–º—É–º: {min_samples}, –ø–æ–ª—É—á–µ–Ω–æ: {len(df)}"
        )

    # --- 6. –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤ ---
    class_ratio = (
        df['actual_status'].value_counts(normalize=True).to_dict()
    )
    imbalance_threshold = 0.1
    if (
        len(class_ratio) == 2 and min(class_ratio.values()) <
            imbalance_threshold
    ):
        raise ValueError(
            f"–°–∏–ª—å–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: {class_ratio}. "
            f"–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–æ–ª—è –æ–¥–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞: {imbalance_threshold}"
        )

    # --- 7. –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ ---
    df_clean = df.drop_duplicates(subset=required_cols[:-1])  # –±–µ–∑ actual_status
    if len(df_clean) < min_samples:
        raise ValueError(
            "–ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö "
            "–¥–ª—è –æ–±—É—á–µ–Ω–∏—è"
        )

    logger.info(f"‚úÖ –û—á–∏—â–µ–Ω–æ: {len(df)} ‚Üí {len(df_clean)} –∑–∞–ø–∏—Å–µ–π")

    # --- 8. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π ---
    X_raw = df_clean[required_cols[:-1]]
    y = df_clean['actual_status']

    # --- 9. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ ---
    X_list = []
    for _, row in X_raw.iterrows():
        row_df = pd.DataFrame([row])
        processed = preprocess_data_for_prediction(row_df)
        X_list.append(processed.iloc[0])
    X = pd.DataFrame(X_list).reset_index(drop=True)

    # --- 10. –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ---
    if FEATURE_NAMES_PATH.exists():
        expected_features = joblib.load(FEATURE_NAMES_PATH)
        logger.info(
            f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –æ–∂–∏–¥–∞–µ–º—ã–µ —Ñ–∏—á–∏: {len(expected_features)}"
        )
    else:
        expected_features = X.columns.tolist()
        joblib.dump(expected_features, FEATURE_NAMES_PATH)
        logger.error(
            f"‚ö†Ô∏è feature_names.pkl –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–æ—Ö—Ä–∞–Ω—ë–Ω –Ω–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ "
            f"–∏–∑ {len(expected_features)} —Ñ–∏—á–µ–π"
        )

    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –∫–∞–∫ 0
    for col in expected_features:
        if col not in X.columns:
            X[col] = 0
    X = X[expected_features]

    # --- 11. –ó–∞–≥—Ä—É–∑–∫–∞ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ ---
    if ENSEMBLE_MODEL_PATH.exists():
        model = joblib.load(ENSEMBLE_MODEL_PATH)
        logger.info(
            f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è: "
            f"{ENSEMBLE_MODEL_PATH}"
        )
    else:
        logger.info("üÜï –°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è –∞–Ω—Å–∞–º–±–ª–µ–≤–∞—è –º–æ–¥–µ–ª—å")
        rf = RandomForestClassifier(n_estimators=100, random_state=42)
        xgb = XGBClassifier(
            use_label_encoder=False,
            eval_metric='logloss',
            random_state=42
        )
        cb = CatBoostClassifier(silent=True, random_state=42)
        model = VotingClassifier(
            estimators=[('rf', rf), ('xgb', xgb), ('cb', cb)],
            voting='soft'  # —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
        )

    # --- 12. –î–æ–æ–±—É—á–µ–Ω–∏–µ ---
    logger.info(f"üöÄ –ù–∞—á–∞–ª–æ –¥–æ–æ–±—É—á–µ–Ω–∏—è –Ω–∞ {len(X)} –ø—Ä–∏–º–µ—Ä–∞—Ö...")
    model.fit(X, y)

    # --- 13. –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ ---
    y_pred = model.predict(X)
    accuracy = accuracy_score(y, y_pred)
    logger.info(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ñ–∏–¥–±—ç–∫–∞—Ö: {accuracy:.3f}")

    # --- 14. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ---
    joblib.dump(model, ENSEMBLE_MODEL_PATH)
    logger.info(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {ENSEMBLE_MODEL_PATH}")

    background_data = X.sample(min(100, len(X)), random_state=42)
    joblib.dump(background_data, BACKGROUND_DATA_PATH)
    logger.info(
        f"üíæ background_data –æ–±–Ω–æ–≤–ª—ë–Ω: {BACKGROUND_DATA_PATH}"
    )

    # --- 15. –í–æ–∑–≤—Ä–∞—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ ---
    return {
        "status": "retrained",
        "samples_used": len(X),
        "model_path": str(ENSEMBLE_MODEL_PATH),
        "accuracy_on_feedback": accuracy,
        "class_balance": class_ratio
    }
